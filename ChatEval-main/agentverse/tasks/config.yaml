task:
  llmeval
data_path:
  ./agentverse/tasks/llm_eval/data/faireval/preprocessed_data/test.json
output_dir:
  ./outputs/llm_eval/multi_role/only_static_assign/faireval/two_turns_sequential/two_different_role/calc_score_comparison_reverse/gpt_35_0301
prompts:
  prompt: &prompt |-
    [Question]
    ${source_text}
    [Fact]
    ${source_fact}
    [The Start of Assistant 1’s Answer]
    ${compared_text_one}
    [The End of Assistant 1’s Answer]
    [The Start of Assistant 2’s Answer]
    ${compared_text_two}
    [The End of Assistant 2’s Answer]
    [The Start of Assistant 3’s Answer]
    ${compared_text_three}
    [The End of Assistant 3’s Answer]
    [The Start of Assistant 4’s Answer]
    ${compared_text_four}
    [The End of Assistant 4’s Answer]
    [The Start of Assistant 5’s Answer]
    ${compared_text_five}
    [The End of Assistant 5’s Answer]
    [System]
    We would like to request your feedback on the performance of five AI assistants in response to the user question displayed above.
    Please consider the helpfulness, relevance, accuracy, and level of detail of their responses.
    There are a few other referee assigned the same task, it's your responsibility to discuss with them and think critically before you make your final judgement.
    Each assistant receives an overall score on a scale of 0 to 5, where a higher score indicates better overall performance.

    ${role_description}

    Now it's your time to talk, please make your talk short and clear, ${agent_name} !

    ${final_prompt}


environment:
  env_type: llm_eval
  max_turns: 6
  rule:
    order:
      type: concurrent
    visibility:
      type: all
    selector:
      type: basic
    updater:
      type: basic
    describer:
      type: basic

agents:
  -
    agent_type: llm_eval_multi
    name: Author
    final_prompt_to_use: |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
      Then, output five lines indicating the scores for Assistant 1,2,3,4, and 5, respectively.

      Remember, please ensure that your scores differ from the previous iterations by re-evaluating specific aspects of the responses!
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      The score of Assistant 1: [score only]
      The score of Assistant 2: [score only]
      The score of Assistant 3: [score only]
      The score of Assistant 4: [score only]
      The score of Assistant 5: [score only]
    role_description: |-
      You are the Author, responsible for evaluating the creative, narrative, and overall imaginative qualities of the responses. Your task is to assess how well each assistant develops unique ideas, ensures coherence, and adds compelling narrative elements. Focus on originality, depth, and how engaging the answers are in terms of storytelling or explanation.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-4o-mini"
      llm_type: gpt-4
      temperature: 0.3
      max_tokens: 512
  -
    agent_type: llm_eval_multi
    name: Critic
    final_prompt_to_use: |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
      Then, output five lines indicating the scores for Assistant 1,2,3,4, and 5, respectively.

      Remember, please ensure that your scores differ from the previous iterations by re-evaluating specific aspects of the responses !
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      The score of Assistant 1: [score only]
      The score of Assistant 2: [score only]
      The score of Assistant 3: [score only]
      The score of Assistant 4: [score only]
      The score of Assistant 5: [score only]
    role_description: |-
      You are the Critic, tasked with analyzing the technical quality of the responses. Your role focuses on assessing grammatical correctness, clarity, conciseness, and the use of precise terminology. Be strict in identifying and penalizing issues such as vague language, redundancy, or lack of coherence. Question the reasoning behind other agents' judgments if necessary.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-3.5-turbo-0125"
      llm_type: gpt-3.5-turbo
      temperature: 0
      max_tokens: 512
  -
    agent_type: llm_eval_multi
    name: Psychologist
    final_prompt_to_use: |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
      Then, output five lines indicating the scores for Assistant 1,2,3,4, and 5, respectively.

      Remember, please ensure that your scores differ from the previous iterations by re-evaluating specific aspects of the responses!
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      The score of Assistant 1: [score only]
      The score of Assistant 2: [score only]
      The score of Assistant 3: [score only]
      The score of Assistant 4: [score only]
      The score of Assistant 5: [score only]
    role_description: |-
      You are the Psychologist, an empathetic and analytical observer. Your role is to assess how well each response demonstrates understanding of human emotions, motivations, and psychological depth. Focus on the tone, sensitivity, and how well the responses address subtle emotional cues or interpersonal dynamics.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-4o"
      llm_type: gpt-4
      temperature: 0.7
      max_tokens: 512

tools: ~
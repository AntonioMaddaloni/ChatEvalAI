task:
  llmeval
data_path:
  ./agentverse/tasks/llm_eval/data/faireval/preprocessed_data/test.json
output_dir:
  ./outputs/llm_eval/multi_role/only_static_assign/faireval/two_turns_sequential/two_different_role/calc_score_comparison_reverse/gpt_35_0301
prompts:
  prompt: &prompt |-
    [Question]
    ${source_text}
    [Fact]
    ${source_fact}
    [The Start of Assistant 1’s Answer]
    ${compared_text_one}
    [The End of Assistant 1’s Answer]
    [The Start of Assistant 2’s Answer]
    ${compared_text_two}
    [The End of Assistant 2’s Answer]
    [The Start of Assistant 3’s Answer]
    ${compared_text_three}
    [The End of Assistant 3’s Answer]
    [The Start of Assistant 4’s Answer]
    ${compared_text_four}
    [The End of Assistant 4’s Answer]
    [The Start of Assistant 5’s Answer]
    ${compared_text_five}
    [The End of Assistant 5’s Answer]
    [System]
    We would like to request your feedback on the performance of five AI assistants in response to the user question displayed above.
    Please consider the helpfulness, relevance, accuracy, and level of detail of their responses.
    There are a few other referee assigned the same task, it's your responsibility to discuss with them and think critically before you make your final judgement.
    Each assistant receives an overall score on a scale of 0 to 5, where a higher score indicates better overall performance.

    ${role_description}

    Now it's your time to talk, please make your talk short and clear, ${agent_name} !

    ${final_prompt}


environment:
  env_type: llm_eval
  max_turns: 4
  rule:
    order:
      type: sequential
    visibility:
      type: all
    selector:
      type: basic
    updater:
      type: basic
    describer:
      type: basic

agents:
  -
    agent_type: llm_eval_multi
    name: Author
    final_prompt_to_use: |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
      Then, output five lines indicating the scores for Assistant 1 and 2, respectively.

      Remember that you are not required to output the same value as other referees !
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      The score of Assistant 1: [score only]
      The score of Assistant 2: [score only]
      The score of Assistant 3: [score only]
      The score of Assistant 4: [score only]
      The score of Assistant 5: [score only]
    role_description: |-
      You are now the Author, one of the creative leads in this task. Your goal is to develop the story, expanding on details and adding compelling narrative elements. Think critically and creatively, taking responsibility for providing a clear and engaging direction for the plot.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-3.5-turbo"
      llm_type: gpt-3.5-turbo
      temperature: 0
      max_tokens: 512
  -
    agent_type: llm_eval_multi
    name: Critic
    final_prompt_to_use: |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
      Then, output five lines indicating the scores for Assistant 1 and 2, respectively.

      Remember that you are not required to output the same value as other referees !
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      The score of Assistant 1: [score only]
      The score of Assistant 2: [score only]
      The score of Assistant 3: [score only]
      The score of Assistant 4: [score only]
      The score of Assistant 5: [score only]
    role_description: |-
      You are now Critic, one of the referees in this task. You will check fluent writing, clear sentences, and good wording in summary writing. Your job is to question others judgement to make sure their judgement is well-considered and offer an alternative solution if two responses are at the same level.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-3.5-turbo"
      llm_type: gpt-3.5-turbo
      temperature: 0
      max_tokens: 512
  -
    agent_type: llm_eval_multi
    name: Psychologist
    final_prompt_to_use: |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
      Then, output five lines indicating the scores for Assistant 1,2,3,4, and 5, respectively.

      Remember that you are not required to output the same value as other referees !
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      The score of Assistant 1: [score only]
      The score of Assistant 2: [score only]
      The score of Assistant 3: [score only]
      The score of Assistant 4: [score only]
      The score of Assistant 5: [score only]
    role_description: |-
      You are now the Psychologist, an analytical and empathetic observer in this task. Your role is to examine the psychological aspects of the situation, focusing on the characters' emotions, behaviors, and motivations. Think critically and empathetically, providing insights that add depth and understanding to the narrative.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-3.5-turbo"
      llm_type: gpt-3.5-turbo
      temperature: 0
      max_tokens: 512

tools: ~